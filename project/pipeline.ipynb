{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "def download(url: str, fname: str, chunk_size=1024):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    print(f'url:{url}')\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(fname, 'wb') as file, tqdm(\n",
    "        desc=fname,\n",
    "        total=total,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "\n",
    "def fetch_html(url:str):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    \n",
    "    return resp.content\n",
    "\n",
    "def fetch_all_hyperlinks(link):\n",
    "    html_content = fetch_html(link)\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    return soup.find_all('a')\n",
    "\n",
    "def read_all_csv_files(csv_directory,**kwargs):\n",
    "    csv_directory = Path(csv_directory)\n",
    "    display(f'Read all csv files in {csv_directory}')\n",
    "    csv_files = [f for f in csv_directory.iterdir() if f.is_file()]\n",
    "\n",
    "    \n",
    "    display(f'Found {len(csv_files)} csv files')\n",
    "\n",
    "    data_frame_list = []\n",
    "    for csv_file in tqdm(csv_files):\n",
    "        data_frame_list.append(pd.read_csv(csv_file,**kwargs))\n",
    "    \n",
    "    display(f'Done read')\n",
    "    \n",
    "    return data_frame_list\n",
    "    \n",
    "\n",
    "\n",
    "if not os.path.exists('../data/download'):\n",
    "    os.makedirs('../data/download')\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    if not os.path.exists(f'../data/download/data{i}'):\n",
    "        os.makedirs(f'../data/download/data{i}')\n",
    "\n",
    "\n",
    "data1_path = '../data/download/data1/data1.csv'\n",
    "data2_directory = '../data/download/data2'\n",
    "data3_directory = '../data/download/data3'\n",
    "data4_directory = '../data/download/data4'\n",
    "data5_path = '../data/download/data5/data5.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data source 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = 'https://nyc3.digitaloceanspaces.com/owid-public/data/co2/owid-co2-data.csv'\n",
    "\n",
    "\n",
    "def extract_data1(link):\n",
    "    try:\n",
    "        save_position = data1_path\n",
    "        display('Extracting data source 1')\n",
    "        download(link,save_position)\n",
    "    except Exception as e:\n",
    "        display('Extraction 1 fails')\n",
    "        display(e)\n",
    "    else:\n",
    "        display('Done Extracting data source 1')\n",
    "\n",
    "extract_data1(link1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data source 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link2 = 'https://opendata.dwd.de/climate_environment/CDC/regional_averages_DE/monthly/air_temperature_mean'\n",
    "\n",
    "\n",
    "def extract_data2(link):\n",
    "    print('extract datasource 2')\n",
    "\n",
    "    try:\n",
    "        a_tags = fetch_all_hyperlinks(link)\n",
    "        csv_names = [a_tag['href'] for a_tag in a_tags if a_tag.text.endswith('.txt')]\n",
    "\n",
    "        display(f'Found {len(csv_names)} CSV files in {link}')\n",
    "        display(csv_names)\n",
    "\n",
    "        for csv_name in csv_names:\n",
    "\n",
    "            download(f'{link}/{csv_name}',f'{data2_directory}/{csv_name}')\n",
    "    except Exception as e:\n",
    "        display('Extraction 2 fails')\n",
    "        display(e)\n",
    "    else:\n",
    "        display('Done extracting data 2')\n",
    "\n",
    "extract_data2(link2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data source 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link3 = 'https://opendata.dwd.de/climate_environment/CDC/regional_averages_DE/monthly/precipitation'\n",
    "\n",
    "\n",
    "def extract_data3(link):\n",
    "    print('extract datasource 3')\n",
    "\n",
    "    try:\n",
    "        a_tags = fetch_all_hyperlinks(link)\n",
    "        csv_names = [a_tag['href'] for a_tag in a_tags if a_tag.text.endswith('.txt')]\n",
    "\n",
    "        display(f'Found {len(csv_names)} CSV files in {link}')\n",
    "        display(csv_names)\n",
    "\n",
    "        for csv_name in csv_names:\n",
    "            download(f'{link}/{csv_name}',f'{data3_directory}/{csv_name}')\n",
    "    except Exception as e:\n",
    "        display('Extraction 3 fails')\n",
    "        display(e)\n",
    "    else:\n",
    "        display('Done extracting data 3')\n",
    "\n",
    "\n",
    "extract_data3(link3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data source 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link4 = 'https://opendata.dwd.de/climate_environment/CDC/derived_germany/soil/monthly/historical'\n",
    "\n",
    "\n",
    "def extract_data4(link):\n",
    "    display('extract datasource 4')\n",
    "\n",
    "    try:\n",
    "        a_tags = fetch_all_hyperlinks(link)\n",
    "        gz_names = [a_tag['href'] for a_tag in a_tags if a_tag['href'].endswith('.gz') and 'v2' in a_tag['href']]\n",
    "\n",
    "        display(f'Found {len(gz_names)} GZ files in {link}')\n",
    "        display(gz_names)\n",
    "\n",
    "        for gz_name in tqdm(gz_names):\n",
    "            time.sleep(1)\n",
    "            download(f'{link}/{gz_name}',f'{data4_directory}/{gz_name}')\n",
    "    except Exception as e:\n",
    "        display('Extraction 4 fails')\n",
    "        display(e)\n",
    "    else:\n",
    "        display('Done extracting data 4')\n",
    "\n",
    "extract_data4(link4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data source 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link5 = 'https://opendata.dwd.de/climate_environment/CDC/derived_germany/soil/monthly/historical/derived_germany_soil_monthly_historical_stations_list.txt'\n",
    "\n",
    "def extract_data5(link):\n",
    "    try:\n",
    "        save_position = data5_path\n",
    "        display('Extracting data source 5')\n",
    "        download(link,save_position)\n",
    "    except Exception as e:\n",
    "        display('Extraction 5 fails')\n",
    "        display(e)\n",
    "    else:\n",
    "        display('Done Extracting data source 5')\n",
    "\n",
    "extract_data5(link5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_prototype = {\n",
    "    'year':'int64','co2':'float64','co2_growth_prct':'float64'\n",
    "}\n",
    "\n",
    "data2_prototype = {\n",
    "    'year':'int64','month':'int64','Brandenburg/Berlin':'float64','Brandenburg':'float64','Baden-Wuerttemberg':'float64',\n",
    "    'Bayern':'float64','Hessen':'float64','Mecklenburg-Vorpommern':'float64','Niedersachsen':'float64','Niedersachsen/Hamburg/Bremen':'float64',\n",
    "    'Nordrhein-Westfalen':'float64','Rheinland-Pfalz':'float64','Schleswig-Holstein':'float64','Saarland':'float64',\n",
    "    'Sachsen':'float64','Sachsen-Anhalt':'float64','Thueringen/Sachsen-Anhalt':'float64','Thueringen':'float64','Deutschland':'float64'\n",
    "}\n",
    "\n",
    "data3_prototype = {\n",
    "    'year':'int64','month':'int64','Brandenburg/Berlin':'float64','Brandenburg':'float64','Baden-Wuerttemberg':'float64',\n",
    "    'Bayern':'float64','Hessen':'float64','Mecklenburg-Vorpommern':'float64','Niedersachsen':'float64','Niedersachsen/Hamburg/Bremen':'float64',\n",
    "    'Nordrhein-Westfalen':'float64','Rheinland-Pfalz':'float64','Schleswig-Holstein':'float64','Saarland':'float64',\n",
    "    'Sachsen':'float64','Sachsen-Anhalt':'float64','Thueringen/Sachsen-Anhalt':'float64','Thueringen':'float64','Deutschland':'float64'\n",
    "}\n",
    "\n",
    "data4_prototype = {\n",
    "    \n",
    "    'Stationsindex':'int64','year':'int64','month':'int64','Mittel von TS05':'float64','Mittel von TS10':'float64','Mittel von TS20':'float64',\n",
    "    'Mittel von TS50':'float64','Mittel von TS100':'float64','Mittel von TSLS05':'float64','Mittel von TSSL05':'float64','Maximum von ZFUMI':'float64',\n",
    "    'Maximum von ZTKMI':'float64','Maximum von ZTUMI':'float64','Mittel von BFGL01_AG':'float64','Mittel von BFGL02_AG':'float64','Mittel von BFGL03_AG':'float64',\n",
    "    'Mittel von BFGL04_AG':'float64','Mittel von BFGL05_AG':'float64','Mittel von BFGL06_AG':'float64','Mittel von BFGS_AG':'float64','Mittel von BFGL_AG':'float64',\n",
    "    'Mittel von BFWS_AG':'float64','Mittel von BFMS_AG':'float64','Mittel von BFML_AG':'float64','Summe von VPGFAO':'float64','Summe von VPGH':'float64','Summe von VRGS_AG':'float64',\n",
    "    'Summe von VRGL_AG':'float64','Summe von VRWS_AG':'float64','Summe von VRWL_AG':'float64','Summe von VRML_AG':'float64'\n",
    "}\n",
    "\n",
    "\n",
    "data5_prototype = {\n",
    "    'Stationsindex':'int64',\n",
    "    'Name':'string',\n",
    "    'Bundesland':'string'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Read all csv files in ../data/download/data2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Found 12 csv files'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 55.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done read'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Read all csv files in ../data/download/data3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Found 12 csv files'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 133.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done read'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Read all csv files in ../data/download/data4'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Found 486 csv files'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486/486 [00:05<00:00, 93.55it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done read'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv(data1_path)\n",
    "df2_list = read_all_csv_files(data2_directory,skiprows=1,delimiter=';')\n",
    "df3_list = read_all_csv_files(data3_directory,skiprows=1,delimiter=';')\n",
    "df4_list = read_all_csv_files(data4_directory,delimiter=';')\n",
    "df5 = pd.read_csv(data5_path,delimiter=';',encoding='ISO 8859-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Hashable\n",
    "\n",
    "def German_to_English(string:str) -> str:\n",
    "\n",
    "    string = string.replace('ä','ae')\n",
    "    string = string.replace('ö','oe')\n",
    "    string = string.replace('ü','ue')\n",
    "    string = string.replace('ß','ss')\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def merge_data_frames(data_frame_list) -> pd.DataFrame:\n",
    "    return pd.concat(data_frame_list,axis=0)\n",
    "\n",
    "def enforce_type(value,dtype):\n",
    "    try:\n",
    "        if dtype == 'int64':\n",
    "            return int(value)\n",
    "        elif dtype == 'float64':\n",
    "            return float(value)\n",
    "        elif dtype == 'string':\n",
    "            return str(value)\n",
    "        else:\n",
    "            return value\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def typing_filter(df,column_dtypes:Hashable):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    for column,dtype in column_dtypes.items():\n",
    "        df[column] = df[column].apply(lambda x: enforce_type(x,dtype))\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def check_prototype(data_prototype,data_frame):\n",
    "\n",
    "    columns = set(data_frame.columns)\n",
    "    data_fields = set(data_prototype.keys())\n",
    "    is_fit =  data_fields.issubset(columns)\n",
    "\n",
    "    if not is_fit:\n",
    "        raise TypeError(f'Expected columns:{data_fields} but actually:{columns}, difference:{data_fields - columns}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23347/2627445018.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Name'] = df['Name'].apply(lambda x: x.strip())\n",
      "/tmp/ipykernel_23347/2627445018.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Bundesland'] = df['Bundesland'].apply(lambda x: German_to_English(x.strip()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_data1(df):\n",
    "\n",
    "    # select the world data\n",
    "    df = df[df['country'] == 'World']\n",
    "    \n",
    "    # select the row whose co2 is not null\n",
    "    # and select [year,co2,co2_growth_prct] from it\n",
    "    df = df[df['co2'].notna()]\n",
    "    df = df[['year','co2','co2_growth_prct']]\n",
    "    \n",
    "    df = df.sort_values(by=['year'])\n",
    "    \n",
    "    #set the first column of co2_growth_prct to 0\n",
    "    df.iloc[0,2] = 0\n",
    "\n",
    "    #check prototype\n",
    "    check_prototype(data1_prototype,df)\n",
    "\n",
    "    # set the typing and filter out those not fitting\n",
    "    df = typing_filter(df,data1_prototype)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_data2(df_list):\n",
    "    df = merge_data_frames(df_list)\n",
    "    \n",
    "    #drop the last error colum\n",
    "    df.drop(df.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "    #rename the column name\n",
    "    df.rename(columns={'Jahr':'year','Monat':'month'},inplace=True)\n",
    "\n",
    "    #drop null rows\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # check prototype\n",
    "    check_prototype(data2_prototype,df)\n",
    "\n",
    "    # set the typing and filter out those not fitting\n",
    "    df = typing_filter(df,data2_prototype)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_data3(df_list):\n",
    "    df = merge_data_frames(df_list)\n",
    "\n",
    "    #drop the last error colum\n",
    "    df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    #rename the column name\n",
    "    df.rename(columns={'Jahr':'year','Monat':'month'},inplace=True)\n",
    "\n",
    "    #drop null rows\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # check prototype\n",
    "    check_prototype(data3_prototype,df)\n",
    "\n",
    "    # set the typing and filter out those not fitting\n",
    "    df = typing_filter(df,data3_prototype)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_data4(df_list):\n",
    "    df = merge_data_frames(df_list)\n",
    "\n",
    "    #drop the last error column\n",
    "    df.drop(df.columns[-2:], axis=1, inplace=True)\n",
    "    \n",
    "    #drop any empty row\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # get time for year and month\n",
    "    df['Jahr'] = df['Monat'] // 100\n",
    "    df['Monat'] = df['Monat'] % 100\n",
    "\n",
    "    #rename the column name\n",
    "    df.rename(columns={'Jahr':'year','Monat':'month'},inplace=True)\n",
    "\n",
    "    # check prototype\n",
    "    check_prototype(data4_prototype,df)\n",
    "\n",
    "    # set the typing and filter out those not fitting\n",
    "    df = typing_filter(df,data4_prototype)\n",
    "\n",
    "    return df\n",
    "def transform_data5(df):\n",
    "\n",
    "    #strip the column name\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    #select only the first,\n",
    "    df =df[['Stationsindex','Name','Bundesland']]\n",
    "\n",
    "    # for name column, remove the space\n",
    "    df['Name'] = df['Name'].apply(lambda x: x.strip())\n",
    "\n",
    "    #for bundesland colum, remove the space and convert German letter to English letter\n",
    "    df['Bundesland'] = df['Bundesland'].apply(lambda x: German_to_English(x.strip()))\n",
    "\n",
    "    #drop any empty row\n",
    "    df = df.dropna()\n",
    "\n",
    "    # check prototype\n",
    "    check_prototype(data5_prototype,df)\n",
    "\n",
    "    # set the typing and filter out those not fitting\n",
    "    df = typing_filter(df,data5_prototype)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = transform_data1(df1)\n",
    "df2 = transform_data2(df2_list)\n",
    "df3 = transform_data3(df3_list)\n",
    "df4 = transform_data4(df4_list)\n",
    "df5 = transform_data5(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_by_stationidx(data_frame4,data_frame5):\n",
    "    data_frame45 = data_frame4.join(data_frame5.set_index('Stationsindex'),how='inner',on='Stationsindex')\n",
    "\n",
    "    return data_frame45\n",
    "\n",
    "# find their common year and month set\n",
    "def find_common_year_month(data_frame2,data_frame3,data_frame45):\n",
    "    \n",
    "    year_month_set2 = set(data_frame2[['year','month']].itertuples(index=False, name=None))\n",
    "    year_month_set3 = set(data_frame3[['year','month']].itertuples(index=False, name=None))\n",
    "    year_month_set45 = set(data_frame45[['year','month']].itertuples(index=False, name=None))\n",
    "\n",
    "    return year_month_set2 & year_month_set3 & year_month_set45\n",
    "\n",
    "def filter_common_year_month(data_frame2,data_frame3,data_frame45):\n",
    "    \n",
    "    common_year_month = find_common_year_month(data_frame2,data_frame3,data_frame45)\n",
    "\n",
    "    mask2 = data_frame2.apply(lambda row: (row['year'],row['month']) in common_year_month,axis=1)\n",
    "    mask3 = data_frame3.apply(lambda row: (row['year'],row['month']) in common_year_month,axis=1)\n",
    "    mask45 = data_frame45.apply(lambda row: (row['year'],row['month']) in common_year_month,axis=1)\n",
    "\n",
    "    return data_frame2[mask2],data_frame3[mask3],data_frame45[mask45]\n",
    "\n",
    "\n",
    "df45 = join_by_stationidx(df4,df5)\n",
    "df2,df3,df45 = filter_common_year_month(df2,df3,df45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_file = '../data/data.sql'\n",
    "\n",
    "if os.path.exists(db_file):\n",
    "    # Delete the file\n",
    "    os.remove(db_file)\n",
    "\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "\n",
    "df1.set_index('year').to_sql('co2',if_exists='replace',index=True,con=conn)\n",
    "df2.set_index(['year','month']).to_sql('temperature',if_exists='replace',index=True,con=conn)\n",
    "df3.set_index(['year','month']).to_sql('precipitation',if_exists='replace',index=True,con=conn)\n",
    "df45.set_index(['Stationsindex','year','month']).to_sql('soil',if_exists='replace',index=True,con=conn)\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
